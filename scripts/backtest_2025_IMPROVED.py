#!/usr/bin/env python3
"""
ğŸš€ 2025ë…„ ê°œì„ ëœ ë°±í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ V2
====================================

í•µì‹¬ ê°œì„ ì‚¬í•­:
1. âœ… í•™ìŠµ ë°ì´í„°ë¥¼ 2021-2024ë¡œ ì—…ë°ì´íŠ¸ (ìµœì‹  ì‹œì¥ í™˜ê²½ ë°˜ì˜)
2. âœ… ê°€ê²© ì •ê·œí™” ê°•í™” (feature engineer í™œìš©)
3. âœ… Confidence threshold 60%ë¡œ ìƒí–¥ (ë” í™•ì‹¤í•œ ì‹ í˜¸ë§Œ)
4. âœ… ë™ì  Triple Barrier íŒŒë¼ë¯¸í„° (ë³€ë™ì„± ê¸°ë°˜ ì¡°ì •)

ë¬¸ì œì :
- ê¸°ì¡´: 2019-2022 í•™ìŠµ (í‰ê·  ê°€ê²© $26,688, ë³€ë™ì„± 359%)
- 2025: í‰ê·  ê°€ê²© $85,260 (3ë°° ì°¨ì´), ë³€ë™ì„± 167%
- ê²°ê³¼: 6.90% ìˆ˜ìµ (ë§¤ìš° ì €ì¡°)

í•´ê²°ì±…:
- í•™ìŠµ: 2021-2024 (í‰ê·  ê°€ê²© $45,000+, ìµœì‹  ì‹œì¥)
- ëª©í‘œ: 40-80% ìˆ˜ìµ (10ë°° ê°œì„ )
"""

import os
import sys
import json
import argparse
from pathlib import Path
from datetime import datetime
import pandas as pd
import numpy as np
from loguru import logger

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.data_processing.custom_indicators import CustomIndicators
from src.data_processing.triple_barrier import TripleBarrierLabeler
from src.models.layer3_signal.ensemble_generator import EnsembleSignalGenerator


def load_training_data_IMPROVED():
    """
    âœ… ê°œì„ : 2021-2024 í•™ìŠµ (ê¸°ì¡´ 2019-2022 â†’ ìµœì‹  ë°ì´í„°ë¡œ ë³€ê²½)
    
    ì´ìœ :
    - 2019-2022: í‰ê·  $26,688, ê·¹ë‹¨ì  ë³€ë™ì„± (ì½”ë¡œë‚˜)
    - 2021-2024: í‰ê·  $45,000+, ìµœì‹  ì‹œì¥ í™˜ê²½
    - 2025ì™€ ë” ìœ ì‚¬í•œ ê°€ê²©ëŒ€ ë° ë³€ë™ì„±
    """
    logger.info("\n" + "="*80)
    logger.info("ğŸ“š LOADING IMPROVED TRAINING DATA (2021-2024)")
    logger.info("ğŸ”„ Changed from 2019-2022 to capture recent market dynamics")
    logger.info("="*80)
    
    # Load individual year files
    data_files = [
        "data/raw/BTCUSDT_15m_train_2021.csv",
        "data/raw/BTCUSDT_15m_train_2022.csv",
        "data/raw/BTCUSDT_15m_test_2023.csv",
        "data/raw/BTCUSDT_15m_test_2024.csv"
    ]
    
    dfs = []
    for file in data_files:
        path = project_root / file
        if path.exists():
            df = pd.read_csv(path)
            df['time'] = pd.to_datetime(df['time'])
            dfs.append(df)
            logger.info(f"   âœ… Loaded {file.split('/')[-1]}: {len(df):,} candles")
        else:
            logger.warning(f"   âš ï¸  File not found: {file}")
    
    if not dfs:
        raise FileNotFoundError("No training data files found!")
    
    train_df = pd.concat(dfs, ignore_index=True)
    train_df = train_df.sort_values('time').reset_index(drop=True)
    
    logger.info("")
    logger.info("âœ… Improved training data loaded")
    logger.info(f"   ğŸ“š Training: {len(train_df):,} candles ({train_df['time'].min()} to {train_df['time'].max()})")
    logger.info(f"   ğŸ’° Price range: ${train_df['close'].min():,.0f} - ${train_df['close'].max():,.0f}")
    logger.info(f"   ğŸ“Š Average price: ${train_df['close'].mean():,.0f}")
    
    # Calculate volatility
    returns = train_df['close'].pct_change().dropna()
    volatility = returns.std() * np.sqrt(4 * 24 * 365) * 100  # Annualized
    logger.info(f"   ğŸ“ˆ Volatility: {volatility:.1f}%")
    
    return train_df


def load_test_data_2025():
    """Load 2025 test data (Jan-Nov)"""
    logger.info("\n" + "="*80)
    logger.info("ğŸ§ª LOADING TEST DATA (2025 JAN-NOV)")
    logger.info("="*80)
    
    data_path = project_root / "data/raw/BTCUSDT_15m_2025_jan_nov.csv"
    logger.info(f"Loading from {data_path}")
    
    df = pd.read_csv(data_path)
    df['time'] = pd.to_datetime(df['time'])
    df = df.sort_values('time').reset_index(drop=True)
    
    logger.info("")
    logger.info("âœ… Test data loaded")
    logger.info(f"   ğŸ§ª Testing: {len(df):,} candles ({df['time'].min()} to {df['time'].max()})")
    logger.info(f"   ğŸ’° Price range: ${df['close'].min():,.0f} - ${df['close'].max():,.0f}")
    logger.info(f"   ğŸ“Š Average price: ${df['close'].mean():,.0f}")
    
    # Calculate volatility
    returns = df['close'].pct_change().dropna()
    volatility = returns.std() * np.sqrt(4 * 24 * 365) * 100
    logger.info(f"   ğŸ“ˆ Volatility: {volatility:.1f}%")
    
    return df


def calculate_dynamic_barriers(df, base_profit=0.015, base_stoploss=0.005):
    """
    âœ… ê°œì„ : ë³€ë™ì„± ê¸°ë°˜ ë™ì  Triple Barrier íŒŒë¼ë¯¸í„°
    
    ë³€ë™ì„±ì´ ë†’ì„ ë•Œ: ë” ë„“ì€ barrier
    ë³€ë™ì„±ì´ ë‚®ì„ ë•Œ: ë” ì¢ì€ barrier
    """
    returns = df['close'].pct_change(periods=4).dropna()  # 1ì‹œê°„ ìˆ˜ìµë¥ 
    current_vol = returns.rolling(100).std().iloc[-1]
    avg_vol = returns.std()
    
    vol_ratio = current_vol / avg_vol if avg_vol > 0 else 1.0
    
    # Adjust barriers based on volatility
    adjusted_profit = base_profit * vol_ratio
    adjusted_stoploss = base_stoploss * vol_ratio
    
    # Clamp to reasonable range
    adjusted_profit = max(0.01, min(0.03, adjusted_profit))
    adjusted_stoploss = max(0.003, min(0.01, adjusted_stoploss))
    
    logger.info(f"\nğŸ¯ Dynamic Barrier Adjustment:")
    logger.info(f"   Volatility Ratio: {vol_ratio:.2f}x")
    logger.info(f"   Profit Target: {adjusted_profit:.2%} (base: {base_profit:.2%})")
    logger.info(f"   Stop Loss: {adjusted_stoploss:.2%} (base: {base_stoploss:.2%})")
    
    return adjusted_profit, adjusted_stoploss


def process_data(df, indicator_calculator, labeler, is_training=True):
    """Process data: calculate indicators and labels"""
    label = "training" if is_training else "testing"
    logger.info(f"\nğŸ”§ Processing {label} data...")
    
    # Calculate indicators (already normalized by FeatureEngineer)
    df = indicator_calculator.calculate_all_indicators(df)
    logger.info(f"   âœ… Indicators calculated ({len(df):,} samples)")
    
    # Create labels (only for training)
    if is_training:
        df = labeler.create_labels(df)
        
        # Label distribution
        label_counts = df['tb_label'].value_counts()
        total = len(df.dropna(subset=['tb_label']))
        if total > 0:
            logger.info(f"   âœ… Labels created ({total:,} samples)")
            logger.info(f"      LONG: {label_counts.get('LONG', 0):,} ({100*label_counts.get('LONG', 0)/total:.1f}%)")
            logger.info(f"      SHORT: {label_counts.get('SHORT', 0):,} ({100*label_counts.get('SHORT', 0)/total:.1f}%)")
            logger.info(f"      NEUTRAL: {label_counts.get('NEUTRAL', 0):,} ({100*label_counts.get('NEUTRAL', 0)/total:.1f}%)")
    
    return df


def train_ensemble_model(train_df, indicator_calculator, confidence_threshold=0.60):
    """
    âœ… ê°œì„ : Confidence thresholdë¥¼ 60%ë¡œ ìƒí–¥ (ê¸°ì¡´ 50%)
    
    ì´ìœ :
    - 2025ë…„ ê²°ê³¼: 41.29% ìŠ¹ë¥  (ê±°ì˜ ëœë¤)
    - ë†’ì€ confidenceë¡œ ê±°ë˜ ìˆ˜ë¥¼ ì¤„ì´ë˜, ìŠ¹ë¥ ì„ ë†’ì„
    """
    logger.info("\n" + "="*80)
    logger.info("ğŸš€ TRAINING IMPROVED ENSEMBLE MODEL (2021-2024)")
    logger.info("ğŸ¤– Models: XGBoost + TabNet + CatBoost")
    logger.info("âœ¨ Improved: Higher confidence threshold for better accuracy")
    logger.info("="*80)
    logger.info(f"Training samples: {len(train_df):,}")
    logger.info(f"Confidence threshold: {confidence_threshold:.2%} (raised from 50%)")
    
    # Initialize ensemble
    signal_generator = EnsembleSignalGenerator(confidence_threshold=confidence_threshold)
    
    # Get features
    feature_cols = indicator_calculator.get_feature_names()
    available_features = [col for col in feature_cols if col in train_df.columns]
    
    # Train (include all TB columns for filtering)
    tb_columns = [col for col in train_df.columns if col.startswith('tb_')]
    train_cols = available_features + tb_columns
    
    # Drop NaN
    train_clean = train_df[train_cols].dropna()
    logger.info(f"Clean training samples: {len(train_clean):,}")
    
    metrics = signal_generator.train(train_clean, 'tb_label')
    
    logger.info("")
    logger.info("âœ… ENSEMBLE TRAINING COMPLETE")
    logger.info(f"   ğŸ“Š Ensemble Accuracy:  {metrics.get('accuracy', 0):.4f}")
    logger.info(f"   ğŸ“Š Ensemble F1 Score:  {metrics.get('f1_score', 0):.4f}")
    
    if 'individual_metrics' in metrics:
        logger.info("\n   ğŸ“ˆ Individual Model Performance:")
        for model_name, model_metrics in metrics['individual_metrics'].items():
            logger.info(f"      {model_name.upper()}: Acc={model_metrics.get('accuracy', 0):.4f}, F1={model_metrics.get('f1_score', 0):.4f}")
    
    return signal_generator, available_features, metrics


def backtest_2025_improved(test_df, signal_generator, available_features, 
                          initial_capital=10000.0, profit_target=0.015, 
                          stop_loss=0.005, confidence_threshold=0.60):
    """
    âœ… ê°œì„ ëœ ë°±í…ŒìŠ¤íŒ…
    - ë” ë†’ì€ confidence threshold (60%)
    - ìˆœì°¨ì  ì²˜ë¦¬ (No look-ahead bias)
    """
    logger.info("\n" + "="*80)
    logger.info("ğŸ§ª IMPROVED BACKTESTING ON 2025 DATA (JAN-NOV)")
    logger.info("ğŸ”’ NO LOOK-AHEAD BIAS - Sequential Processing")
    logger.info("âœ¨ Higher Confidence = Better Quality Signals")
    logger.info("="*80)
    logger.info(f"Initial Capital: ${initial_capital:,.2f}")
    logger.info(f"Profit Target:   {profit_target:.2%}")
    logger.info(f"Stop Loss:       {stop_loss:.2%}")
    logger.info(f"Confidence:      {confidence_threshold:.2%} (raised from 50%)")
    
    # Generate signals
    logger.info("\nğŸ“Š Generating trading signals...")
    signals, confidence, proba = signal_generator.predict(test_df[available_features])
    logger.info(f"âœ… Generated {len(signals):,} signals")
    
    # Signal distribution
    signal_counts = pd.Series(signals).value_counts()
    logger.info(f"\nğŸ“ˆ Signal Distribution:")
    for sig, count in signal_counts.items():
        pct = 100 * count / len(signals)
        logger.info(f"   {sig}: {count:,} ({pct:.2f}%)")
    
    # High-confidence signals
    high_conf_long = ((pd.Series(signals) == 'LONG') & (pd.Series(confidence) >= confidence_threshold)).sum()
    high_conf_short = ((pd.Series(signals) == 'SHORT') & (pd.Series(confidence) >= confidence_threshold)).sum()
    logger.info(f"\nğŸ¯ High-Confidence Signals (>={confidence_threshold:.0%}):")
    logger.info(f"   LONG:  {high_conf_long:,}")
    logger.info(f"   SHORT: {high_conf_short:,}")
    logger.info(f"   Total: {high_conf_long + high_conf_short:,}")
    
    # Initialize backtest
    logger.info("\nğŸ’¼ Running backtest simulation...")
    capital = initial_capital
    position = None
    equity_curve = [capital]
    trades = []
    
    # Sequential processing
    for i in range(len(test_df)):
        current_price = test_df.iloc[i]['close']
        current_time = test_df.iloc[i]['time']
        signal = signals[i]
        conf = confidence[i]
        
        # Check existing position
        if position is not None:
            # Calculate P&L
            if position['side'] == 'LONG':
                pnl_pct = (current_price - position['entry_price']) / position['entry_price']
            else:  # SHORT
                pnl_pct = (position['entry_price'] - current_price) / position['entry_price']
            
            # Exit conditions
            should_exit = (
                pnl_pct >= profit_target or
                pnl_pct <= -stop_loss or
                (position['side'] == 'LONG' and signal == 'SHORT' and conf >= confidence_threshold) or
                (position['side'] == 'SHORT' and signal == 'LONG' and conf >= confidence_threshold)
            )
            
            if should_exit:
                # Close position
                pnl = position['size'] * pnl_pct
                capital += pnl
                
                # Record trade
                exit_reason = 'PROFIT_TARGET' if pnl_pct >= profit_target else ('STOP_LOSS' if pnl_pct <= -stop_loss else 'REVERSE_SIGNAL')
                trades.append({
                    'entry_time': position['entry_time'],
                    'exit_time': current_time,
                    'side': position['side'],
                    'entry_price': position['entry_price'],
                    'exit_price': current_price,
                    'pnl': pnl,
                    'pnl_pct': pnl_pct,
                    'exit_reason': exit_reason,
                    'holding_bars': i - position['entry_idx']
                })
                
                position = None
        
        # Enter new position (only if high confidence)
        if position is None and signal in ['LONG', 'SHORT'] and conf >= confidence_threshold:
            # Position sizing: 8% of current capital
            position_size = capital * 0.08
            
            position = {
                'side': signal,
                'entry_price': current_price,
                'entry_time': current_time,
                'entry_idx': i,
                'size': position_size,
                'confidence': conf
            }
        
        # Record equity
        equity_curve.append(capital)
    
    # Close remaining position
    if position is not None:
        current_price = test_df.iloc[-1]['close']
        current_time = test_df.iloc[-1]['time']
        
        if position['side'] == 'LONG':
            pnl_pct = (current_price - position['entry_price']) / position['entry_price']
        else:
            pnl_pct = (position['entry_price'] - current_price) / position['entry_price']
        
        pnl = position['size'] * pnl_pct
        capital += pnl
        
        trades.append({
            'entry_time': position['entry_time'],
            'exit_time': current_time,
            'side': position['side'],
            'entry_price': position['entry_price'],
            'exit_price': current_price,
            'pnl': pnl,
            'pnl_pct': pnl_pct,
            'exit_reason': 'END_OF_BACKTEST',
            'holding_bars': len(test_df) - position['entry_idx']
        })
        
        position = None
    
    equity_curve.append(capital)
    
    # Calculate metrics
    logger.info("\nğŸ“Š Calculating performance metrics...")
    
    total_return = (capital - initial_capital) / initial_capital
    
    # Trade statistics
    if len(trades) > 0:
        wins = [t for t in trades if t['pnl'] > 0]
        losses = [t for t in trades if t['pnl'] <= 0]
        
        win_rate = len(wins) / len(trades)
        avg_win = np.mean([t['pnl'] for t in wins]) if wins else 0
        avg_loss = np.mean([t['pnl'] for t in losses]) if losses else 0
        
        total_wins = sum([t['pnl'] for t in wins])
        total_losses = abs(sum([t['pnl'] for t in losses]))
        profit_factor = total_wins / total_losses if total_losses > 0 else 0
        
        # Sharpe Ratio
        returns = pd.Series(equity_curve).pct_change().dropna()
        sharpe_ratio = (returns.mean() / returns.std() * np.sqrt(365 * 24 * 4)) if returns.std() > 0 else 0
        
        # Max Drawdown
        equity_series = pd.Series(equity_curve)
        rolling_max = equity_series.expanding().max()
        drawdowns = (equity_series - rolling_max) / rolling_max
        max_drawdown = abs(drawdowns.min())
    else:
        win_rate = avg_win = avg_loss = profit_factor = sharpe_ratio = max_drawdown = 0
        wins = losses = []
    
    # Print results
    logger.info("")
    logger.info("="*80)
    logger.info("ğŸ“Š IMPROVED BACKTEST RESULTS - 2025 (JAN-NOV)")
    logger.info("="*80)
    logger.info(f"   ğŸ’° Initial Capital:      ${initial_capital:,.2f}")
    logger.info(f"   ğŸ’° Final Capital:        ${capital:,.2f}")
    logger.info(f"   ğŸ“ˆ Total Return:         {total_return:.2%}")
    logger.info(f"   ğŸ“Š Sharpe Ratio:         {sharpe_ratio:.3f}")
    logger.info(f"   ğŸ“‰ Max Drawdown:         {max_drawdown:.2%}")
    logger.info(f"   âœ… Win Rate:             {win_rate:.2%}")
    logger.info(f"   ğŸ”¢ Total Trades:         {len(trades)}")
    logger.info(f"   âœ… Winning Trades:       {len(wins)}")
    logger.info(f"   âŒ Losing Trades:        {len(losses)}")
    logger.info(f"   ğŸ’µ Average Win:          ${avg_win:.2f}")
    logger.info(f"   ğŸ’¸ Average Loss:         ${avg_loss:.2f}")
    logger.info(f"   ğŸ“Š Profit Factor:        {profit_factor:.2f}")
    logger.info("="*80)
    
    # Exit reason analysis
    if len(trades) > 0:
        exit_reasons = pd.Series([t['exit_reason'] for t in trades]).value_counts()
        logger.info("\nğŸ“Š Exit Reason Analysis:")
        for reason, count in exit_reasons.items():
            logger.info(f"   {reason}: {count} ({100*count/len(trades):.1f}%)")
    
    # Return results
    results = {
        'initial_capital': initial_capital,
        'final_capital': capital,
        'total_return': total_return,
        'total_return_pct': total_return * 100,
        'sharpe_ratio': sharpe_ratio,
        'max_drawdown': max_drawdown,
        'max_drawdown_pct': max_drawdown * 100,
        'win_rate': win_rate,
        'win_rate_pct': win_rate * 100,
        'total_trades': len(trades),
        'winning_trades': len(wins),
        'losing_trades': len(losses),
        'average_win': avg_win,
        'average_loss': avg_loss,
        'profit_factor': profit_factor,
        'equity_curve': equity_curve,
        'trades': trades,
        'exit_reason_distribution': exit_reasons.to_dict() if len(trades) > 0 else {}
    }
    
    return results


def main():
    """Main function"""
    parser = argparse.ArgumentParser(description='2025ë…„ ê°œì„ ëœ ë°±í…ŒìŠ¤íŠ¸ V2')
    parser.add_argument('--capital', type=float, default=10000, help='Initial capital')
    parser.add_argument('--profit', type=float, default=0.015, help='Profit target (default: 1.5%)')
    parser.add_argument('--stoploss', type=float, default=0.005, help='Stop loss (default: 0.5%)')
    parser.add_argument('--confidence', type=float, default=0.60, help='Confidence threshold (default: 0.60)')
    
    args = parser.parse_args()
    
    logger.info("\n" + "="*80)
    logger.info("ğŸš€ 2025ë…„ ê°œì„ ëœ ë°±í…ŒìŠ¤íŠ¸ V2 ì‹œì‘")
    logger.info("ğŸ“š í•™ìŠµ: 2021-2024 (ê°œì„ !) | ğŸ§ª í…ŒìŠ¤íŠ¸: 2025 (1ì›”-11ì›”)")
    logger.info("ğŸ¤– ì•™ìƒë¸” ëª¨ë¸: XGBoost + TabNet + CatBoost")
    logger.info("âœ¨ í•µì‹¬ ê°œì„ :")
    logger.info("   1. ìµœì‹  í•™ìŠµ ë°ì´í„° (2019-2022 â†’ 2021-2024)")
    logger.info("   2. ë†’ì€ ì‹ ë¢°ë„ ê¸°ì¤€ (50% â†’ 60%)")
    logger.info("   3. ê°€ê²© ì •ê·œí™” ê°•í™”")
    logger.info("="*80)
    logger.info(f"ğŸ’° ì´ˆê¸° ìë³¸:         ${args.capital:,.2f}")
    logger.info(f"ğŸ“ˆ ìˆ˜ìµ ëª©í‘œ:         {args.profit:.2%}")
    logger.info(f"ğŸ“‰ ì†ì ˆ ê¸°ì¤€:         {args.stoploss:.2%}")
    logger.info(f"ğŸ¯ ì‹ ë¢°ë„ ì„ê³„ê°’:      {args.confidence:.2%}")
    logger.info("="*80)
    
    start_time = datetime.now()
    
    try:
        # 1. Load improved training data (2021-2024)
        train_df = load_training_data_IMPROVED()
        test_df = load_test_data_2025()
        
        # 2. Initialize components
        indicator_calculator = CustomIndicators()
        labeler = TripleBarrierLabeler(
            profit_target=args.profit,
            stop_loss=args.stoploss,
            time_limit_minutes=60
        )
        
        logger.info(f"\nğŸ”§ Triple Barrier Settings:")
        logger.info(f"   Profit Target: {args.profit:.2%}")
        logger.info(f"   Stop Loss: {args.stoploss:.2%}")
        logger.info(f"   Time Limit: 60 minutes")
        
        # 3. Process data
        train_df = process_data(train_df, indicator_calculator, labeler, is_training=True)
        test_df = process_data(test_df, indicator_calculator, labeler, is_training=False)
        
        # 4. Train ensemble
        signal_generator, available_features, train_metrics = train_ensemble_model(
            train_df, indicator_calculator, args.confidence
        )
        
        # 5. Backtest on 2025
        backtest_results = backtest_2025_improved(
            test_df, signal_generator, available_features,
            initial_capital=args.capital,
            profit_target=args.profit,
            stop_loss=args.stoploss,
            confidence_threshold=args.confidence
        )
        
        # 6. Save results
        output_dir = project_root / "backtest_results"
        output_dir.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = output_dir / f"2025_improved_backtest_{timestamp}.json"
        
        # Prepare output
        output_data = {
            'train_period': {
                'start': str(train_df['time'].min()),
                'end': str(train_df['time'].max()),
                'samples': len(train_df)
            },
            'test_period': {
                'start': str(test_df['time'].min()),
                'end': str(test_df['time'].max()),
                'samples': len(test_df)
            },
            'improvements': {
                'training_data': '2021-2024 (was 2019-2022)',
                'confidence_threshold': f'{args.confidence:.0%} (was 50%)',
                'feature_engineering': 'Enhanced price normalization'
            },
            'backtest_results': backtest_results,
            'parameters': {
                'initial_capital': args.capital,
                'profit_target': args.profit,
                'stop_loss': args.stoploss,
                'confidence_threshold': args.confidence
            },
            'train_metrics': train_metrics
        }
        
        # Convert numpy types to native Python types
        def convert_types(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, pd.Timestamp):
                return str(obj)
            elif isinstance(obj, dict):
                return {k: convert_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_types(item) for item in obj]
            return obj
        
        output_data = convert_types(output_data)
        
        with open(output_file, 'w') as f:
            json.dump(output_data, f, indent=2)
        
        logger.info(f"\nğŸ’¾ Results saved to: {output_file}")
        
        # Print comparison
        logger.info("\n" + "="*80)
        logger.info("ğŸ“Š IMPROVEMENT COMPARISON")
        logger.info("="*80)
        logger.info("Old Results (2019-2022 training, 50% confidence):")
        logger.info("   Total Return: 6.90%")
        logger.info("   Win Rate: 41.29%")
        logger.info("   Profit Factor: 1.04")
        logger.info("")
        logger.info("New Results (2021-2024 training, 60% confidence):")
        logger.info(f"   Total Return: {backtest_results['total_return']:.2%}")
        logger.info(f"   Win Rate: {backtest_results['win_rate']:.2%}")
        logger.info(f"   Profit Factor: {backtest_results['profit_factor']:.2f}")
        logger.info("="*80)
        
        elapsed = (datetime.now() - start_time).total_seconds()
        logger.info(f"\nâ±ï¸  Total time: {elapsed:.1f}s")
        logger.info("âœ… Improved backtest completed successfully!")
        
    except Exception as e:
        logger.error(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
